{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLKaWygjXN-V",
        "outputId": "facfcaa3-8b97-4329-d84d-0ead0018d804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset saved to customer_service_fine_tune_data.jsonl\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load your dataset\n",
        "file_path = 'Bitext_Sample_Customer_Support_Training_Dataset_27K_responses-v11.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Transform the dataset into the required format\n",
        "jsonl_data = []\n",
        "for _, row in df.iterrows():\n",
        "    entry = {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful e-commerce customer service assistant. Provide clear, accurate and stylish answers and relevant product recommendations.\"},\n",
        "            {\"role\": \"user\", \"content\": row[\"instruction\"]},\n",
        "            {\"role\": \"assistant\", \"content\": row[\"response\"]}\n",
        "        ]\n",
        "    }\n",
        "    jsonl_data.append(entry)\n",
        "\n",
        "# Save as JSONL\n",
        "output_file = 'customer_service_fine_tune_data.jsonl'\n",
        "with open(output_file, 'w') as f:\n",
        "    for entry in jsonl_data:\n",
        "        f.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "print(f\"Dataset saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QedEAPLYeUg9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uno3R5mgrQsm"
      },
      "outputs": [],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwDSG2caebcF",
        "outputId": "4da80914-438a-4860-9104-00158f69fa76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples: 26872\n",
            "First example:\n",
            "{'role': 'system', 'content': 'You are a helpful e-commerce customer service assistant. Provide clear, accurate and stylish answers and relevant product recommendations.'}\n",
            "{'role': 'user', 'content': 'question about cancelling order {{Order Number}}'}\n",
            "{'role': 'assistant', 'content': \"I've understood you have a question regarding canceling order {{Order Number}}, and I'm here to provide you with the information you need. Please go ahead and ask your question, and I'll do my best to assist you.\"}\n"
          ]
        }
      ],
      "source": [
        "data_path = \"customer_service_fine_tune_data.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SuU05EHetrj",
        "outputId": "39975cdb-089a-4033-faa6-3d536f48f78c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found\n"
          ]
        }
      ],
      "source": [
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1wYCsQhfARq"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rLhHDjjfHo1",
        "outputId": "5b084118-4227-4e4b-c6cd-c12b9a35b79d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 55, 527\n",
            "mean / median: 172.25520988389403, 151.5\n",
            "p5 / p95: 113.0, 261.0\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 12, 478\n",
            "mean / median: 125.24006400714498, 104.0\n",
            "p5 / p95: 67.0, 214.0\n",
            "\n",
            "0 examples may be over the 16,385 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ],
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 16385 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 16,385 token limit, they will be truncated during fine-tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxqJEIyTgWtt",
        "outputId": "e4eb6341-e3e7-4df6-e617-0571ea2599e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has ~4628842 tokens that will be charged for during training\n",
            "By default, you'll train for 1 epochs on this dataset\n",
            "By default, you'll be charged for ~4628842 tokens\n"
          ]
        }
      ],
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 16385\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyGjkD9mgqTQ"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n",
        "client.files.create(\n",
        "  file=open(\"customer_service_fine_tune_data.jsonl\", \"rb\"),\n",
        "  purpose=\"fine-tune\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7hJIkk1jdDX",
        "outputId": "fb9ce5f2-a3c0-4b4d-bb4f-c4f576a61935"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FineTuningJob(id='ftjob-BZwIipxudjU2dXTo0AXsvCKI', created_at=1724188804, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-Hh0nIE9drk4sJaBIhtI2Lkj9', result_files=[], seed=431097433, status='validating_files', trained_tokens=None, training_file='file-Kwqa8M0tR7cMCSbWgDRXZuHd', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix=None)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.fine_tuning.jobs.create(\n",
        "  training_file=\"file-Kwqa8M0tR7cMCSbWgDRXZuHd\",\n",
        "  model=\"gpt-4o-mini-2024-07-18\",\n",
        "   hyperparameters={\n",
        "    \"n_epochs\":3\n",
        "  }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AqSunrmoWF8"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"\")\n",
        "\n",
        "\n",
        "# Initialize conversation history\n",
        "\n",
        "system_prompt=\"\"\" You are a specialized e-commerce customer service assistant. Your primary role is to assist with questions related to products, orders, shipping, returns, and customer support inquiries. Please avoid answering questions unrelated to e-commerce or customer service. Provide clear, accurate, and stylish answers while offering relevant product recommendations.\"\"\"\n",
        "conversation_history = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhPkXjvI6VoD",
        "outputId": "20a463f7-b500-4832-c9f6-2c10d0dea2c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi boss\n",
            "Thank you for reaching out! While I appreciate the friendly greeting, I would like to remind you that I am here to assist you with your inquiries related to our company. Please let me know if you have any questions or concerns, and I'll be more than happy to help you.\n",
            "who r u ?\n",
            "I appreciate your curiosity! My purpose is to assist you in any way I can. I am here to provide you with information, support, and guidance related to our products and services. Your satisfaction and well-being are of utmost importance and I am dedicated to making your experience with us exceptional. Please let me know if there is anything specific you would like assistance with.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "UserMessage=\"who r u ?\"\n",
        "conversation_history.append({\"role\": \"user\", \"content\":UserMessage })\n",
        "\n",
        "# Generate assistant's response\n",
        "completion = client.chat.completions.create(\n",
        "    model=\"ft:gpt-4o-mini-2024-07-18:personal::9ySAnm7D\",\n",
        "    messages=conversation_history\n",
        ")\n",
        "\n",
        "response = completion.choices[0].message.content\n",
        "\n",
        "# Update conversation history\n",
        "conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "for message in conversation_history[1:]:\n",
        "    print(message['content'])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
